{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lGBWHzTdTPmz",
        "WI44TAJKJynA",
        "nLQMFlETNd32",
        "jslfSUYhLSQ4",
        "jl-Wl32Q_Eon",
        "N9j1D34AsVIt",
        "oN3sKcPumA5z",
        "Kg91rFULFzDU",
        "EK92yHdkRu0s",
        "UNWu8qT-SK7t",
        "w1p9BqsTUQ9E",
        "d7MxCkJbXVvz",
        "YIK6-sOk2YEn",
        "zjJhz4o-uiWa"
      ],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPGmIyit6tIZ0VsuyHG8WbS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NydiaLiu98/Transport_AI_Course/blob/main/project/project_LSTM_GCNLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqXL6qsqaBf-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch, numpy as np, random\n",
        "# seed = 42\n",
        "# torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)"
      ],
      "metadata": {
        "id": "hmPYJasn0OE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('training_dataset.csv',sep=\";\")\n",
        "df"
      ],
      "metadata": {
        "id": "CJCGxfV_lhy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "x0txfxXWaLk9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_W7kMM5fesq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['Date'].astype(str)+\" \"+df['Time'],\n",
        "                  format='%Y%m%d %H:%M:%S')"
      ],
      "metadata": {
        "id": "jQRLzyNbaPj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df1 = pd.read_csv(\"final_evaluation_dataset.csv\",sep=\";\")\n",
        "test_df2 = pd.read_csv(\"evaluation_dataset.csv\",sep=\";\")\n",
        "combined_df = pd.concat([test_df1, test_df2], ignore_index=True)\n",
        "combined_df = combined_df.drop_duplicates(subset=[\"DP_ID\", \"PORTAL\", \"Date\", \"Time\"])\n",
        "combined_df = combined_df.sort_values(by=[\"PORTAL\", \"Date\", \"Time\"]).reset_index(drop=True)\n",
        "combined_df.to_csv(\"combined_evaluation_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "tt3awEohhSVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"combined_evaluation_dataset.csv\",sep=\",\")\n",
        "test_df['datetime'] = pd.to_datetime(test_df['Date'].astype(str)+\" \"+test_df['Time'],\n",
        "                  format='%Y%m%d %H:%M:%S')"
      ],
      "metadata": {
        "id": "-esIytQbGRE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Portal Aggregation"
      ],
      "metadata": {
        "id": "lGBWHzTdTPmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_start = pd.Timestamp('2021-06-01 00:00:00')\n",
        "global_end = pd.Timestamp('2021-12-31 23:59:00')\n",
        "full_index = pd.date_range(start=global_start, end=global_end, freq='1min')\n",
        "mask = (full_index.time >= pd.to_datetime(\"04:00:00\").time()) & (full_index.time <= pd.to_datetime(\"10:00:00\").time())\n",
        "peak_index = full_index[mask]"
      ],
      "metadata": {
        "id": "y3vwOvnNdHl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_start = pd.Timestamp('2022-01-01 00:00:00')\n",
        "test_end = pd.Timestamp('2022-06-30 23:59:00')\n",
        "test_full_index = pd.date_range(start=test_start, end=test_end, freq='1min')\n",
        "mask = (test_full_index.time >= pd.to_datetime(\"04:00:00\").time()) & (test_full_index.time <= pd.to_datetime(\"10:00:00\").time())\n",
        "test_peak_index = test_full_index[mask]"
      ],
      "metadata": {
        "id": "AHEaa2-1Gi8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_missing_minutes(df,dp_id,peak_index=peak_index):\n",
        "\n",
        "  d = (df.loc[df[\"DP_ID\"] == dp_id, [\"datetime\", \"FLOW\", \"SPEED_MS_AVG\", \"PORTAL\"]]\n",
        "           .dropna(subset=[\"datetime\"])\n",
        "           .sort_values(\"datetime\")\n",
        "           .set_index(\"datetime\"))\n",
        "\n",
        "  d = d[~d.index.duplicated(keep=\"first\")]\n",
        "  d = d.loc[(d.index >= peak_index.min()) & (d.index <= peak_index.max())]\n",
        "\n",
        "  missing_index = peak_index.difference(d.index)\n",
        "\n",
        "  actual_minutes = len(d)\n",
        "  expected_minutes = len(peak_index)\n",
        "  missing_minutes = len(missing_index)\n",
        "  missing_ratio = missing_minutes / expected_minutes * 100\n",
        "\n",
        "  return {\n",
        "        \"DP_ID\": dp_id,\n",
        "        \"expected_minutes\": expected_minutes,\n",
        "        \"actual_minutes\": actual_minutes,\n",
        "        \"missing_minutes\": missing_minutes,\n",
        "        \"missing_ratio(%)\": round(missing_ratio, 2)\n",
        "    }\n",
        "\n",
        "def check_all_dp_missing(df,peak_index=peak_index):\n",
        "    result = []\n",
        "    for dp in sorted(df[\"DP_ID\"].unique()):\n",
        "        result.append(check_missing_minutes(df, dp, peak_index=peak_index))\n",
        "    return pd.DataFrame(result).sort_values(\"missing_ratio(%)\", ascending=False)\n",
        "\n",
        "check = check_all_dp_missing(df)\n",
        "print(check)"
      ],
      "metadata": {
        "id": "WzKpN8_8aoW3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_check = check_all_dp_missing(test_df,peak_index=test_peak_index)\n",
        "print(test_check)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qlsbdL2LHHei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dp_full(df, dp_id, peak_index=peak_index):\n",
        "    d = (df.loc[df[\"DP_ID\"]==dp_id, [\"datetime\",\"FLOW\",\"SPEED_MS_AVG\",\"PORTAL\"]]\n",
        "           .dropna(subset=[\"datetime\"])\n",
        "           .sort_values(\"datetime\")\n",
        "           .set_index(\"datetime\"))\n",
        "    d = d[~d.index.duplicated(keep=\"first\")]\n",
        "    d = d.loc[(d.index>=peak_index.min()) & (d.index<=peak_index.max())]\n",
        "\n",
        "    d = d.reindex(peak_index)\n",
        "    d.index.name = \"datetime\"\n",
        "\n",
        "    d[\"PORTAL\"] = df.loc[df[\"DP_ID\"]==dp_id, \"PORTAL\"].dropna().iloc[0]\n",
        "    return d\n",
        "\n",
        "def make_all_dp_full(df,peak_index=peak_index):\n",
        "    result = []\n",
        "    for dp in sorted(df[\"DP_ID\"].unique()):\n",
        "      dp_full = make_dp_full(df,dp,peak_index=peak_index)\n",
        "      dp_full[\"DP_ID\"] = dp\n",
        "      result.append(dp_full)\n",
        "\n",
        "    all_dp_full = pd.concat(result)\n",
        "    all_dp_full.index.name = \"datetime\"\n",
        "    return all_dp_full\n",
        "\n",
        "all_dp_full = make_all_dp_full(df)\n",
        "all_dp_full"
      ],
      "metadata": {
        "id": "PbDn5rtXa9tF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_test_dp_full = make_all_dp_full(test_df,peak_index=test_peak_index)\n",
        "all_test_dp_full"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YacLrqTfH6Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dp_full.isna().sum()"
      ],
      "metadata": {
        "id": "UZzMuGfvGxL-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_test_dp_full.isna().sum()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1ok9i5OSH_qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grp = all_dp_full.groupby([\"PORTAL\",\"datetime\"],sort=True)\n",
        "\n",
        "# aggregate flow (sum)\n",
        "flow_agg = grp[\"FLOW\"].sum(min_count=1).rename(\"FLOW\")\n",
        "\n",
        "# aggregate speed （weighted)\n",
        "def weighted_speed(g):\n",
        "  speed = g[\"SPEED_MS_AVG\"]\n",
        "  flow = g[\"FLOW\"]\n",
        "  mask = speed.notna() & flow.notna() & (flow>0)\n",
        "  if mask.any():\n",
        "    return np.average(speed[mask], weights=flow[mask])\n",
        "  else:\n",
        "    return np.nan\n",
        "\n",
        "speed_agg = grp.apply(weighted_speed).rename(\"SPEED_MS_AVG\")\n",
        "\n",
        "portal_minute = pd.concat([flow_agg,speed_agg],axis=1).reset_index()\n",
        "portal_minute"
      ],
      "metadata": {
        "id": "bui2xbfsfzUc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portal_minute.to_csv(\"portal_minute.csv\", index=False)"
      ],
      "metadata": {
        "id": "4MI8PsRYKBGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_grp = all_test_dp_full.groupby([\"PORTAL\",\"datetime\"],sort=True)\n",
        "\n",
        "# aggregate flow (sum)\n",
        "flow_agg = test_grp[\"FLOW\"].sum(min_count=1).rename(\"FLOW\")\n",
        "# aggregate speed (weighted)\n",
        "speed_agg = test_grp.apply(weighted_speed).rename(\"SPEED_MS_AVG\")\n",
        "portal_minute_test = pd.concat([flow_agg,speed_agg],axis=1).reset_index()\n",
        "portal_minute_test"
      ],
      "metadata": {
        "id": "_laHTx4eIm5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portal_minute_test.to_csv(\"portal_minute_test.csv\", index=False)"
      ],
      "metadata": {
        "id": "muBzSYMCLtX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portal_minute.info()"
      ],
      "metadata": {
        "id": "cFGBCdQKO9Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portal_minute.isna().sum()"
      ],
      "metadata": {
        "id": "rQlwXyXErK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portal_minute_test.info()"
      ],
      "metadata": {
        "id": "7IqgFBYHI83w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portal_minute_test.isna().sum()"
      ],
      "metadata": {
        "id": "1FRWIaMPJD7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive Analysis (Before Imputation)"
      ],
      "metadata": {
        "id": "WI44TAJKJynA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pm = pd.read_csv(\"portal_minute.csv\")\n",
        "pm_test = pd.read_csv(\"portal_minute_test.csv\")\n",
        "pm[\"datetime\"] = pd.to_datetime(pm[\"datetime\"])\n",
        "pm_test[\"datetime\"] = pd.to_datetime(pm_test[\"datetime\"])"
      ],
      "metadata": {
        "id": "FZH_7-i45f3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fow & Speed Distribution (Hist)"
      ],
      "metadata": {
        "id": "nLQMFlETNd32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.dpi\"] = 150\n",
        "sns.set(context=\"notebook\", style=\"whitegrid\")"
      ],
      "metadata": {
        "id": "ZsEZx7SBOapQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,2.5))\n",
        "pm[\"FLOW\"].dropna().plot(kind=\"hist\", bins=50)\n",
        "plt.xlabel(\"FLOW\")\n",
        "plt.title(\"Histogram of FLOW\")\n",
        "plt.savefig(\"hist_flow.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "TlPe2LDFNdEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,2.5))\n",
        "pm[\"SPEED_MS_AVG\"].dropna().plot(kind=\"hist\", bins=50)\n",
        "plt.xlabel(\"SPEED_MS_AVG (m/s)\")\n",
        "plt.title(\"Histogram of SPEED\")\n",
        "plt.savefig(\"hist_speed.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "MqnIo_FpNupz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temporal Analysis"
      ],
      "metadata": {
        "id": "jslfSUYhLSQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_time_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "  d = df.copy()\n",
        "  d[\"year\"] = d[\"datetime\"].dt.year\n",
        "  d[\"month\"] = d[\"datetime\"].dt.month\n",
        "  d[\"day\"] = d[\"datetime\"].dt.day\n",
        "  d[\"hour\"] = d[\"datetime\"].dt.hour\n",
        "  d[\"minute\"] = d[\"datetime\"].dt.minute\n",
        "  d[\"dow\"] = d[\"datetime\"].dt.dayofweek\n",
        "  d[\"is_weekend\"] = (d[\"dow\"] >= 5)\n",
        "  return d"
      ],
      "metadata": {
        "id": "bfXCHj0UKc5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def temporal_analysis(df:pd.DataFrame):\n",
        "  d = add_time_cols(df)\n",
        "\n",
        "  # Minute-level intraday average curve (FLOW)\n",
        "  flow_minute = d.dropna(subset=[\"FLOW\"]).groupby([\"hour\",\"minute\"])[\"FLOW\"].mean().reset_index()\n",
        "  flow_minute[\"tminute\"] = flow_minute[\"hour\"]*60 + flow_minute[\"minute\"]\n",
        "\n",
        "  plt.figure(figsize=(6,2.2))\n",
        "  plt.plot(flow_minute[\"tminute\"], flow_minute[\"FLOW\"], linewidth=1.0)\n",
        "  plt.xticks([240,300,360,420,480,540,600], [\"04:00\",\"05:00\",\"06:00\",\"07:00\",\"08:00\",\"09:00\",\"10:00\"], rotation=0)\n",
        "  plt.xlabel(\"Time of day\")\n",
        "  plt.ylabel(\"Flow (mean)\")\n",
        "  plt.title(\"Minute-level average FLOW\")\n",
        "  plt.savefig(\"minute_level_flow.png\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Minute-level intraday average curve (SPEED)\n",
        "  speed_minute = d.dropna(subset=[\"SPEED_MS_AVG\"]).groupby([\"hour\",\"minute\"])[\"SPEED_MS_AVG\"].mean().reset_index()\n",
        "  speed_minute[\"tminute\"] = speed_minute[\"hour\"]*60 + speed_minute[\"minute\"]\n",
        "  plt.figure(figsize=(6,2.2))\n",
        "  plt.plot(speed_minute[\"tminute\"], speed_minute[\"SPEED_MS_AVG\"], linewidth=1.0)\n",
        "  plt.xticks([240,300,360,420,480,540,600], [\"04:00\",\"05:00\",\"06:00\",\"07:00\",\"08:00\",\"09:00\",\"10:00\"], rotation=0)\n",
        "  plt.xlabel(\"Time of day\")\n",
        "  plt.ylabel(\"Speed (m/s, mean)\")\n",
        "  plt.title(\"Minute-level average SPEED\")\n",
        "  plt.savefig(\"minute_level_speed\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Hour-level intraday average curve (FLOW)\n",
        "  flow_hour = d.dropna(subset=[\"FLOW\"]).groupby([\"hour\"])[\"FLOW\"].mean().reset_index()\n",
        "  plt.figure(figsize=(6,2.2))\n",
        "  plt.plot(flow_hour[\"hour\"], flow_hour[\"FLOW\"], marker=\"o\", linewidth=1.0)\n",
        "  plt.xticks([4,5,6,7,8,9,10], [\"04:00\",\"05:00\",\"06:00\",\"07:00\",\"08:00\",\"09:00\",\"10:00\"], rotation=0)\n",
        "  plt.xlabel(\"Time of day\")\n",
        "  plt.ylabel(\"Flow (mean)\")\n",
        "  plt.title(\"Hour-level average FLOW\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Hour-level intraday average curve (SPEED)\n",
        "  speed_hour = d.dropna(subset=[\"SPEED_MS_AVG\"]).groupby([\"hour\"])[\"SPEED_MS_AVG\"].mean().reset_index()\n",
        "  plt.figure(figsize=(6,2.2))\n",
        "  plt.plot(speed_hour[\"hour\"], speed_hour[\"SPEED_MS_AVG\"], marker=\"o\", linewidth=1.0)\n",
        "  plt.xticks([4,5,6,7,8,9,10], [\"04:00\",\"05:00\",\"06:00\",\"07:00\",\"08:00\",\"09:00\",\"10:00\"], rotation=0)\n",
        "  plt.xlabel(\"Time of day\")\n",
        "  plt.ylabel(\"Speed (m/s, mean)\")\n",
        "  plt.title(\"Hour-level average SPEED\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  ### Distinguish between weekdays and weekends\n",
        "  # Minute-level intraday average curve (FLOW)\n",
        "  wd = d[~d[\"is_weekend\"]]\n",
        "  we = d[d[\"is_weekend\"]]\n",
        "\n",
        "  flow_minute_wd = wd.dropna(subset=[\"FLOW\"]).groupby([\"hour\",\"minute\"])[\"FLOW\"].mean().reset_index()\n",
        "  flow_minute_wd[\"tminute\"] = flow_minute_wd[\"hour\"]*60 + flow_minute_wd[\"minute\"]\n",
        "\n",
        "  flow_minute_we = we.dropna(subset=[\"FLOW\"]).groupby([\"hour\",\"minute\"])[\"FLOW\"].mean().reset_index()\n",
        "  flow_minute_we[\"tminute\"] = flow_minute_we[\"hour\"]*60 + flow_minute_we[\"minute\"]\n",
        "\n",
        "  plt.figure(figsize=(6,2.2))\n",
        "  plt.plot(flow_minute_wd[\"tminute\"], flow_minute_wd[\"FLOW\"], linewidth=1.0, label=\"weekday\")\n",
        "  plt.plot(flow_minute_we[\"tminute\"], flow_minute_we[\"FLOW\"], linewidth=1.0, label=\"weekend\")\n",
        "  plt.xticks([240,300,360,420,480,540,600], [\"04:00\",\"05:00\",\"06:00\",\"07:00\",\"08:00\",\"09:00\",\"10:00\"], rotation=0)\n",
        "  plt.xlabel(\"Time of day\")\n",
        "  plt.ylabel(\"Flow (mean)\")\n",
        "  plt.title(\"Minute-level average FLOW: Weekday vs Weekend\")\n",
        "  plt.legend()\n",
        "  plt.savefig(\"flow_weekday_weekend.png\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Minute-level intraday average curve (SPEED)\n",
        "  flow_minute_wd = wd.dropna(subset=[\"SPEED_MS_AVG\"]).groupby([\"hour\",\"minute\"])[\"SPEED_MS_AVG\"].mean().reset_index()\n",
        "  flow_minute_wd[\"tminute\"] = flow_minute_wd[\"hour\"]*60 + flow_minute_wd[\"minute\"]\n",
        "\n",
        "  flow_minute_we = we.dropna(subset=[\"SPEED_MS_AVG\"]).groupby([\"hour\",\"minute\"])[\"SPEED_MS_AVG\"].mean().reset_index()\n",
        "  flow_minute_we[\"tminute\"] = flow_minute_we[\"hour\"]*60 + flow_minute_we[\"minute\"]\n",
        "\n",
        "  plt.figure(figsize=(6,2.2))\n",
        "  plt.plot(flow_minute_wd[\"tminute\"], flow_minute_wd[\"SPEED_MS_AVG\"], linewidth=1.0, label=\"weekday\")\n",
        "  plt.plot(flow_minute_we[\"tminute\"], flow_minute_we[\"SPEED_MS_AVG\"], linewidth=1.0, label=\"weekend\")\n",
        "  plt.xticks([240,300,360,420,480,540,600], [\"04:00\",\"05:00\",\"06:00\",\"07:00\",\"08:00\",\"09:00\",\"10:00\"], rotation=0)\n",
        "  plt.xlabel(\"Time of day\")\n",
        "  plt.ylabel(\"Speed (m/s, mean)\")\n",
        "  plt.title(\"Minute-level average SPEED: Weekday vs. Weekend\")\n",
        "  plt.legend()\n",
        "  plt.savefig(\"speed_weekday_weekend.png\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Weakly Heatmap\n",
        "  piv_flow = d.pivot_table(index=\"dow\", columns=\"hour\", values=\"FLOW\", aggfunc=\"mean\")\n",
        "  piv_speed = d.pivot_table(index=\"dow\", columns=\"hour\", values=\"SPEED_MS_AVG\", aggfunc=\"mean\")\n",
        "  plt.figure(figsize=(6.5,2.5))\n",
        "  sns.heatmap(piv_flow, cmap=\"viridis\", cbar_kws={\"label\":\"FLOW mean\"})\n",
        "  plt.title(\"Week-Hour heatmap (FLOW)\"); plt.xlabel(\"Hour\"); plt.ylabel(\"Day of Week (0=Mon)\")\n",
        "  plt.savefig(\"flow_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout()\n",
        "  plt.figure(figsize=(6.5,2.5))\n",
        "  sns.heatmap(piv_speed, cmap=\"viridis\", cbar_kws={\"label\":\"Speed mean (m/s)\"})\n",
        "  plt.title(\"Weekly-Hour heatmap (SPEED)\"); plt.xlabel(\"Hour\"); plt.ylabel(\"Day of Week (0=Mon)\")\n",
        "  plt.savefig(\"speed_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Monthly\n",
        "  flow_month = d.dropna(subset=[\"FLOW\"]).groupby([\"year\",\"month\"])[\"FLOW\"].mean().reset_index()\n",
        "  flow_month[\"ym\"] = flow_month[\"year\"].astype(\"str\") + \"-\" + flow_month[\"month\"].astype(\"str\")\n",
        "\n",
        "  plt.figure(figsize=(6,2.2))\n",
        "  plt.plot(flow_month[\"ym\"], flow_month[\"FLOW\"], linewidth=1.0)\n",
        "  plt.xticks(rotation=0)\n",
        "  plt.xlabel(\"Month\")\n",
        "  plt.ylabel(\"Flow (mean)\")\n",
        "  plt.title(\"Monthly average FLOW\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  speed_month = d.dropna(subset=[\"SPEED_MS_AVG\"]).groupby([\"year\",\"month\"])[\"SPEED_MS_AVG\"].mean().reset_index()\n",
        "  speed_month[\"ym\"] = speed_month[\"year\"].astype(\"str\") + \"-\" + speed_month[\"month\"].astype(\"str\")\n",
        "  plt.figure(figsize=(6,2.2))\n",
        "  plt.plot(speed_month[\"ym\"], speed_month[\"SPEED_MS_AVG\"], linewidth=1.0)\n",
        "  plt.xticks(rotation=0)\n",
        "  plt.xlabel(\"Month\")\n",
        "  plt.ylabel(\"Speed (m/s, mean)\")\n",
        "  plt.title(\"Monthly average SPEED\")\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "g8nnwT77LM1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temporal_analysis(pm)"
      ],
      "metadata": {
        "id": "1E-YyrtMODBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spatial Analysis"
      ],
      "metadata": {
        "id": "jl-Wl32Q_Eon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spatial_analysis(df:pd.DataFrame):\n",
        "  d = df.copy()\n",
        "\n",
        "  # Flow by portal\n",
        "  plt.figure(figsize=(7,6))\n",
        "  plt.subplot(2,1,1)\n",
        "  sns.boxplot(data=d.dropna(subset=[\"FLOW\"]), x=\"PORTAL\", y=\"FLOW\", fliersize=1)\n",
        "  plt.title(\"FLOW by PORTAL\")\n",
        "  plt.xlabel(\"\")\n",
        "  plt.ylabel(\"Flow (mean)\")\n",
        "  plt.xticks(rotation=30, ha=\"right\")\n",
        "  plt.savefig(\"flow_portal.png\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Speed by portal\n",
        "  plt.subplot(2,1,2)\n",
        "  sns.boxplot(data=d.dropna(subset=[\"SPEED_MS_AVG\"]), x=\"PORTAL\", y=\"SPEED_MS_AVG\", fliersize=1)\n",
        "  plt.title(\"SPEED by PORTAL\")\n",
        "  plt.xlabel(\" \")\n",
        "  plt.ylabel(\"Speed (m/s, mean)\")\n",
        "  plt.xticks(rotation=30, ha=\"right\")\n",
        "  plt.savefig(\"speed_portal.png\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Consider downstreams and upstreams\n",
        "  portal_ids = [\"E4S 55,620\", \"E4S 56,160\", \"E4S 56,490\", \"E4S 56,780\",\n",
        "          \"E4S 57,055\", \"E4S 57,435\", \"E4S 57,820\", \"E4S 58,140\"]\n",
        "\n",
        "  flow_portal = d.dropna(subset=[\"FLOW\"]).groupby(\"PORTAL\")[\"FLOW\"].mean().reindex(portal_ids)\n",
        "  plt.figure(figsize=(6.5,2.2))\n",
        "  plt.plot(flow_portal.index, flow_portal.values, marker=\"o\")\n",
        "  plt.title(\"FLOW gradient along river (downstream → upstream)\")\n",
        "  plt.xticks(rotation=30, ha=\"right\")\n",
        "  plt.ylabel(\"FLOW mean\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  speed_portal = d.dropna(subset=[\"SPEED_MS_AVG\"]).groupby(\"PORTAL\")[\"SPEED_MS_AVG\"].mean().reindex(portal_ids)\n",
        "  plt.figure(figsize=(6.5,2.2))\n",
        "  plt.plot(speed_portal.index, speed_portal.values, marker=\"o\")\n",
        "  plt.title(\"SPEED gradient along river (downstream → upstream)\")\n",
        "  plt.xticks(rotation=30, ha=\"right\")\n",
        "  plt.ylabel(\"Speed mean (m/s)\")\n",
        "  plt.tight_layout()\n",
        "  # Results from these two graphs can be obtained from boxplots, so can delete these two line charts\n",
        "\n",
        "  #\n",
        "  pivot_flow = d.pivot_table(index=\"datetime\", columns=\"PORTAL\", values=\"FLOW\", aggfunc=\"mean\")\n",
        "  pivot_speed = d.pivot_table(index=\"datetime\", columns=\"PORTAL\", values=\"SPEED_MS_AVG\", aggfunc=\"mean\")\n",
        "\n",
        "  corr_flow = pivot_flow.corr()\n",
        "  corr_speed = pivot_speed.corr()\n",
        "\n",
        "  plt.figure(figsize=(5,4))\n",
        "  sns.heatmap(corr_flow, cmap=\"coolwarm\", vmin=-1, vmax=1, square=True)\n",
        "  plt.title(\"Correlation matrix (FLOW)\")\n",
        "  plt.savefig(\"flow_cmatrix.png\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.figure(figsize=(5,4))\n",
        "  sns.heatmap(corr_speed, cmap=\"coolwarm\", vmin=-1, vmax=1, square=True)\n",
        "  plt.title(\"Correlation matrix (SPEED)\")\n",
        "  plt.savefig(\"speed_cmatrix.png\", dpi=300, bbox_inches=\"tight\")\n",
        "  plt.tight_layout();"
      ],
      "metadata": {
        "id": "mW0N9-a4_Hk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spatial_analysis(pm)"
      ],
      "metadata": {
        "id": "CupBWch9DArB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Values"
      ],
      "metadata": {
        "id": "N9j1D34AsVIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = portal_minute.sort_values([\"PORTAL\",\"datetime\"]).copy()\n",
        "\n",
        "# split training and validation set\n",
        "VAL_START = pd.Timestamp(\"2021-12-01 04:00:00\")\n",
        "VAL_END = pd.Timestamp(\"2021-12-31 10:00:00\")\n",
        "\n",
        "train_df = df[df[\"datetime\"] < VAL_START]\n",
        "val_df = df[(df[\"datetime\"] >= VAL_START) & (df[\"datetime\"] <= VAL_END)]\n",
        "test_df = portal_minute_test.sort_values([\"PORTAL\",\"datetime\"]).copy()\n",
        "\n",
        "# calculate statistics on training set (in order to avoid information leakage)\n",
        "train_df[\"Hour\"] = train_df[\"datetime\"].dt.hour\n",
        "train_df[\"Minute\"] = train_df[\"datetime\"].dt.minute\n",
        "# same portal same hour & minute: flow & speed median\n",
        "train_pt_flow = train_df.groupby([\"PORTAL\",\"Hour\",\"Minute\"])[\"FLOW\"].median()\n",
        "train_pt_speed = train_df.groupby([\"PORTAL\",\"Hour\",\"Minute\"])[\"SPEED_MS_AVG\"].median()\n",
        "\n",
        "# first fill with train_df\n",
        "train_df.set_index(\"datetime\", inplace=True)\n",
        "train_df[\"SPEED_MS_AVG\"] = (train_df.groupby(\"PORTAL\")[\"SPEED_MS_AVG\"]\n",
        "                   .transform(lambda s: s.interpolate(method=\"time\", limit=2))\n",
        ")\n",
        "train_df.reset_index(inplace=True)\n",
        "\n",
        "train_df = train_df.merge(train_pt_speed, on=[\"PORTAL\",\"Hour\",\"Minute\"], how=\"left\", suffixes=(\"\", \"_median\"))\n",
        "train_df[\"SPEED_MS_AVG\"] = train_df[\"SPEED_MS_AVG\"].fillna(train_df[\"SPEED_MS_AVG_median\"])\n",
        "\n",
        "train_df = train_df.merge(train_pt_flow, on=[\"PORTAL\",\"Hour\",\"Minute\"], how=\"left\", suffixes=(\"\", \"_median\"))\n",
        "train_df[\"FLOW\"] = train_df[\"FLOW\"].fillna(train_df[\"FLOW_median\"])\n",
        "train_df[\"FLOW\"] = train_df[\"FLOW\"].fillna(0.0)\n",
        "\n",
        "# Speed: time-based interpolation within each PORTAL for short gaps(<=2 mins)\n",
        "val_df.set_index(\"datetime\",inplace=True)\n",
        "val_df[\"SPEED_MS_AVG\"] = (val_df.groupby(\"PORTAL\")[\"SPEED_MS_AVG\"]\n",
        "                 .transform(lambda s: s.interpolate(method=\"time\", limit=2)))\n",
        "val_df.reset_index(inplace=True)\n",
        "\n",
        "test_df.set_index(\"datetime\",inplace=True)\n",
        "test_df[\"SPEED_MS_AVG\"] = (test_df.groupby(\"PORTAL\")[\"SPEED_MS_AVG\"]\n",
        "                 .transform(lambda s: s.interpolate(method=\"time\", limit=2)))\n",
        "test_df.reset_index(inplace=True)\n",
        "\n",
        "# Speed: for remaining nans, fill the historical median of training set at the same portal and same time\n",
        "val_df[\"Hour\"] = val_df[\"datetime\"].dt.hour\n",
        "val_df[\"Minute\"] = val_df[\"datetime\"].dt.minute\n",
        "val_df = val_df.merge(train_pt_speed, on=[\"PORTAL\",\"Hour\",\"Minute\"], how=\"left\", suffixes=(\"\", \"_median\"))\n",
        "val_df[\"SPEED_MS_AVG\"] = val_df[\"SPEED_MS_AVG\"].fillna(val_df[\"SPEED_MS_AVG_median\"])\n",
        "\n",
        "test_df[\"Hour\"] = test_df[\"datetime\"].dt.hour\n",
        "test_df[\"Minute\"] = test_df[\"datetime\"].dt.minute\n",
        "test_df = test_df.merge(train_pt_speed, on=[\"PORTAL\",\"Hour\",\"Minute\"], how=\"left\", suffixes=(\"\", \"_median\"))\n",
        "test_df[\"SPEED_MS_AVG\"] = test_df[\"SPEED_MS_AVG\"].fillna(test_df[\"SPEED_MS_AVG_median\"])\n",
        "\n",
        "# Flow: first, fill the historical median of training set at the same portal and same time\n",
        "val_df = val_df.merge(train_pt_flow, on=[\"PORTAL\",\"Hour\",\"Minute\"], how=\"left\", suffixes=(\"\", \"_median\"))\n",
        "val_df[\"FLOW\"] = val_df[\"FLOW\"].fillna(val_df[\"FLOW_median\"])\n",
        "\n",
        "test_df = test_df.merge(train_pt_flow, on=[\"PORTAL\",\"Hour\",\"Minute\"], how=\"left\", suffixes=(\"\", \"_median\"))\n",
        "test_df[\"FLOW\"] = test_df[\"FLOW\"].fillna(test_df[\"FLOW_median\"])\n",
        "\n",
        "# Flow: for remaining nans, fill 0\n",
        "val_df[\"FLOW\"] = val_df[\"FLOW\"].fillna(0.0)\n",
        "test_df[\"FLOW\"] = test_df[\"FLOW\"].fillna(0.0)\n",
        "\n",
        "val_df.drop([\"Hour\",\"Minute\",\"SPEED_MS_AVG_median\",\"FLOW_median\"],axis=1,inplace=True)\n",
        "test_df.drop([\"Hour\",\"Minute\",\"SPEED_MS_AVG_median\",\"FLOW_median\"],axis=1,inplace=True)\n",
        "train_df.drop([\"Hour\",\"Minute\",\"SPEED_MS_AVG_median\",\"FLOW_median\"],axis=1,inplace=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NmGbnbt2izLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(\"train_df.csv\",index=False)\n",
        "val_df.to_csv(\"val_df.csv\",index=False)\n",
        "test_df.to_csv(\"test_df.csv\",index=False)"
      ],
      "metadata": {
        "id": "A-hyuQ4ds_QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive Analysis (After Imputation/Verify the rationality of interpolation）"
      ],
      "metadata": {
        "id": "oN3sKcPumA5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pm_filled = pd.concat([train_df, val_df])"
      ],
      "metadata": {
        "id": "DW1w_rA8mRM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,2.5))\n",
        "pm_filled[\"FLOW\"].plot(kind=\"hist\", bins=50)\n",
        "plt.xlabel(\"FLOW\")\n",
        "plt.title(\"Histogram of FLOW\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "puzxA7lBmlsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,2.5))\n",
        "pm_filled[\"SPEED_MS_AVG\"].plot(kind=\"hist\", bins=50)\n",
        "plt.xlabel(\"SPEED_MS_AVG (m/s)\")\n",
        "plt.title(\"Histogram of SPEED\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "smjBZLWbmtbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temporal_analysis(pm_filled)"
      ],
      "metadata": {
        "id": "oJEobCOom7cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spatial_analysis(pm_filled)"
      ],
      "metadata": {
        "id": "AzKqxEjknN3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Help Functions"
      ],
      "metadata": {
        "id": "Kg91rFULFzDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Setup device agnostic code**"
      ],
      "metadata": {
        "id": "EK92yHdkRu0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "ookVcFVqR0fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Create a function to Time our experiments**"
      ],
      "metadata": {
        "id": "UNWu8qT-SK7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float, end: float, device: torch.device = device):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "2EeFY35GSQyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Functionize the training and test loop**"
      ],
      "metadata": {
        "id": "w1p9BqsTUQ9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model:torch.nn.Module,\n",
        "        data_loader:torch.utils.data.DataLoader,\n",
        "        loss_fn:torch.nn.Module,\n",
        "        optimizer:torch.optim.Optimizer,\n",
        "        device:torch.device=device):\n",
        "\n",
        "  train_loss = 0\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  for batch, (X,y) in enumerate(data_loader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    y_pred = model(X)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(lstm_all.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(data_loader)\n",
        "\n",
        "  print(f\"Train loss: {train_loss:.5f}\")\n",
        "  return train_loss\n",
        "\n",
        "\n",
        "def test_step(model:torch.nn.Module,\n",
        "        data_loader:torch.utils.data.DataLoader,\n",
        "        loss_fn:torch.nn.Module,\n",
        "        device:torch.device=device):\n",
        "\n",
        "  test_loss = 0\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X,y in data_loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      test_pred = model(X)\n",
        "      test_loss += loss_fn(test_pred, y).item()\n",
        "\n",
        "    test_loss /= len(data_loader)\n",
        "\n",
        "  print(f\"Test loss: {test_loss:.5f}\")\n",
        "  return test_loss"
      ],
      "metadata": {
        "id": "rJAqFI9NWCON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Functionize Evaluation**"
      ],
      "metadata": {
        "id": "d7MxCkJbXVvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
        "\n",
        "def eval_model(model: torch.nn.Module,\n",
        "        data_loader: torch.utils.data.DataLoader,\n",
        "        loss_fn: torch.nn.Module,\n",
        "        device: torch.device = device,\n",
        "        scaler=None,\n",
        "        horizons:list=None,\n",
        "        feature_names:list=None):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    y_true_list = []\n",
        "    y_pred_list= []\n",
        "    loss = 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      for X, y in data_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_pred = model(X)\n",
        "        loss += loss_fn(y_pred, y).item()\n",
        "        y_true_list.append(y.cpu().numpy())\n",
        "        y_pred_list.append(y_pred.cpu().numpy())\n",
        "\n",
        "\n",
        "    loss /= len(data_loader)\n",
        "\n",
        "    metrics = {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "          \"model_loss\": loss}\n",
        "\n",
        "    y_true = np.concatenate(y_true_list, axis=0)\n",
        "    y_pred = np.concatenate(y_pred_list, axis=0)\n",
        "\n",
        "    # inverse tranform\n",
        "    if scaler is not None:\n",
        "      if y_true.ndim ==3:  # [N,H,D]\n",
        "        N, H, D = y_true.shape\n",
        "        y_true_inv = scaler.inverse_transform(y_true.reshape(-1,D)).reshape(N,H,D)\n",
        "        y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1,D)).reshape(N,H,D)\n",
        "      else:\n",
        "        N, H = y_true.shape  # [N,H]\n",
        "        y_true_inv = scaler.inverse_transform(y_true.reshape(-1,1)).reshape(N,H)\n",
        "        y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1,1)).reshape(N,H)\n",
        "    else:\n",
        "      y_true_inv, y_pred_inv = y_true, y_pred\n",
        "\n",
        "    # metrics\n",
        "    if y_true_inv.ndim == 3:\n",
        "      N, H, D = y_true_inv.shape\n",
        "      fnames = feature_names[:D]\n",
        "\n",
        "      # overall\n",
        "      metrics[\"MAE_overall\"] = mean_absolute_error(y_true_inv.ravel(), y_pred_inv.ravel())\n",
        "      metrics[\"RMSE_overall\"] = root_mean_squared_error(y_true_inv.ravel(), y_pred_inv.ravel())\n",
        "\n",
        "      # per feature (flow / speed)\n",
        "      metrics[\"MAE_per_feature\"] = {fnames[i]: mean_absolute_error(y_true_inv[:,:,i].ravel(), y_pred_inv[:,:,i].ravel()) for i in range(D)}\n",
        "      metrics[\"RMSE_per_feature\"] = {fnames[i]: root_mean_squared_error(y_true_inv[:,:,i].ravel(), y_pred_inv[:,:,i].ravel()) for i in range(D)}\n",
        "\n",
        "      # per step (range(horizons))\n",
        "      mae_per_step  = [mean_absolute_error(y_true_inv[:,i-1,:].ravel(), y_pred_inv[:,i-1,:].ravel()) for i in horizons]\n",
        "      rmse_per_step = [root_mean_squared_error(y_true_inv[:,i-1,:].ravel(), y_pred_inv[:,i-1,:].ravel()) for i in horizons]\n",
        "      metrics[\"MAE_per_step\"]  = dict(zip(horizons, mae_per_step))\n",
        "      metrics[\"RMSE_per_step\"] = dict(zip(horizons, rmse_per_step))\n",
        "\n",
        "      # —— Aggregate column names with portal into basic features (FLOW/SPEED.MS.AVG) ——\n",
        "      def _base_feat(name: str):\n",
        "        if name.startswith(\"FLOW\"):\n",
        "          return \"FLOW\"\n",
        "        if name.startswith(\"SPEED_MS_AVG\") or name.startswith(\"SPEED\"):\n",
        "          return \"SPEED_MS_AVG\"\n",
        "          return name.split(\"_\")[0]\n",
        "\n",
        "      groups = {}  # {'FLOW': [j1,j2,...], 'SPEED_MS_AVG': [k1,k2,...]}\n",
        "      for j, name in enumerate(fnames):\n",
        "        groups.setdefault(_base_feat(name), []).append(j)\n",
        "\n",
        "      # —— Calculate MAE/RMSE for 'By Feature x By Step (Summarize All Portals) ——\n",
        "      mae_fps, rmse_fps = {}, {}\n",
        "      rows = []\n",
        "      for feat, js in groups.items():\n",
        "        mae_fps[feat], rmse_fps[feat] = {}, {}\n",
        "        for idx, h in enumerate(horizons):\n",
        "          yt = np.concatenate([y_true_inv[:, idx, j].ravel() for j in js])\n",
        "          yp = np.concatenate([y_pred_inv[:, idx, j].ravel() for j in js])\n",
        "          mae_v  = mean_absolute_error(yt, yp)\n",
        "          rmse_v = root_mean_squared_error(yt, yp)\n",
        "\n",
        "          mae_fps[feat][h]  = mae_v\n",
        "          rmse_fps[feat][h] = rmse_v\n",
        "          rows.append({\"feature\": feat, \"horizon\": h, \"MAE\": mae_v, \"RMSE\": rmse_v})\n",
        "\n",
        "      metrics[\"MAE_per_feature_per_step\"]  = mae_fps\n",
        "      metrics[\"RMSE_per_feature_per_step\"] = rmse_fps\n",
        "      metrics[\"per_step_df\"] = pd.DataFrame(rows)\n",
        "\n",
        "    else:  # [N,H]\n",
        "      N, H = y_true_inv.shape\n",
        "      mae_per_step  = [mean_absolute_error(y_true_inv[:,i-1].ravel(), y_pred_inv[:,i-1].ravel()) for i in horizons]\n",
        "      rmse_per_step = [root_mean_squared_error(y_true_inv[:,i-1].ravel(), y_pred_inv[:,i-1].ravel()) for i in horizons]\n",
        "\n",
        "      mae_mean  = np.mean(mae_per_step)\n",
        "      rmse_mean = np.mean(rmse_per_step)\n",
        "\n",
        "      metrics.update({\n",
        "            \"MAE_mean\": mae_mean,\n",
        "            \"RMSE_mean\": rmse_mean,\n",
        "            \"MAE_per_step\": mae_per_step,\n",
        "            \"RMSE_per_step\": rmse_per_step,\n",
        "      })\n",
        "\n",
        "    return metrics, y_true_inv, y_pred_inv"
      ],
      "metadata": {
        "id": "IplPDIP-Xug1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All-Portals LSTM"
      ],
      "metadata": {
        "id": "YIK6-sOk2YEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLSTM(nn.Module):\n",
        "  def __init__(self,\n",
        "         input_size=2,   # 2 features: flow and speed\n",
        "         hidden_size=64,\n",
        "         num_layers=1,\n",
        "         horizon=15,    # prediction_length: len(HORIZIONS)\n",
        "         out_size=2):    # 2 predict features: flow and speed\n",
        "\n",
        "    super().__init__()\n",
        "    self.horizon=horizon\n",
        "    self.out_size=out_size\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=input_size,\n",
        "               hidden_size=hidden_size,\n",
        "               num_layers=num_layers,\n",
        "               batch_first=True)\n",
        "\n",
        "    self.linear = nn.Linear(in_features=hidden_size,\n",
        "                 out_features=horizon*out_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "      x: [batch_size, seq_len, input_size],\n",
        "\n",
        "      return: [batch_size, horizon, out_size]\n",
        "    \"\"\"\n",
        "\n",
        "    lstm_out, (h_n,c_n) = self.lstm(x)\n",
        "    output = self.linear(lstm_out[:,-1,:])   # many to one\n",
        "\n",
        "    return output.view(-1,self.horizon,self.out_size)"
      ],
      "metadata": {
        "id": "Ky26Hhymf17s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train_df.csv\")\n",
        "val_df = pd.read_csv(\"val_df.csv\")\n",
        "test_df = pd.read_csv(\"test_df.csv\")\n",
        "train_df[\"datetime\"] = pd.to_datetime(train_df[\"datetime\"])\n",
        "val_df[\"datetime\"] = pd.to_datetime(val_df[\"datetime\"])\n",
        "test_df[\"datetime\"] = pd.to_datetime(test_df[\"datetime\"])"
      ],
      "metadata": {
        "id": "FJISYUMqOe5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.sort_values([\"datetime\",\"PORTAL\"], inplace=True)\n",
        "train_flow = train_df.pivot(index=\"datetime\", columns=\"PORTAL\", values=\"FLOW\").add_prefix(\"FLOW_\")\n",
        "train_speed = train_df.pivot(index=\"datetime\", columns=\"PORTAL\", values=\"SPEED_MS_AVG\").add_prefix(\"SPEED_\")\n",
        "train_flow_speed = pd.concat([train_flow, train_speed], axis=1)\n",
        "\n",
        "portal_ids = [\"E4S 55,620\", \"E4S 56,160\", \"E4S 56,490\", \"E4S 56,780\",\n",
        "        \"E4S 57,055\", \"E4S 57,435\", \"E4S 57,820\", \"E4S 58,140\"]\n",
        "feature_cols = []\n",
        "for pid in portal_ids:\n",
        "  for col in (f\"FLOW_{pid}\", f\"SPEED_{pid}\"):\n",
        "    feature_cols.append(col)\n",
        "\n",
        "train_flow_speed = train_flow_speed[feature_cols]"
      ],
      "metadata": {
        "id": "G_O39DOFfznC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.sort_values([\"datetime\",\"PORTAL\"], inplace=True)\n",
        "val_flow = val_df.pivot(index=\"datetime\", columns=\"PORTAL\", values=\"FLOW\").add_prefix(\"FLOW_\")\n",
        "val_speed = val_df.pivot(index=\"datetime\", columns=\"PORTAL\", values=\"SPEED_MS_AVG\").add_prefix(\"SPEED_\")\n",
        "val_flow_speed = pd.concat([val_flow, val_speed], axis=1)\n",
        "val_flow_speed = val_flow_speed[feature_cols]"
      ],
      "metadata": {
        "id": "NVqahQPjhUuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.sort_values([\"datetime\",\"PORTAL\"], inplace=True)\n",
        "test_flow = test_df.pivot(index=\"datetime\", columns=\"PORTAL\", values=\"FLOW\").add_prefix(\"FLOW_\")\n",
        "test_speed = test_df.pivot(index=\"datetime\", columns=\"PORTAL\", values=\"SPEED_MS_AVG\").add_prefix(\"SPEED_\")\n",
        "test_flow_speed = pd.concat([test_flow, test_speed], axis=1)\n",
        "test_flow_speed = test_flow_speed[feature_cols]"
      ],
      "metadata": {
        "id": "wEGkrZOXhqob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_flow_speed.shape, val_flow_speed.shape, test_flow_speed.shape"
      ],
      "metadata": {
        "id": "Jvt7Q_XEjDHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "HORIZONS = list(range(1,16))\n",
        "LOOKBACK = 60\n",
        "BATCH = 32\n",
        "HIDDEN = 96\n",
        "LR = 5e-4\n",
        "WD = 1e-4\n",
        "EPOCHS = 50\n",
        "\n",
        "def make_dataset(data, lookback=60, horizons=None, stride=1):\n",
        "\n",
        "    X, y = [], []\n",
        "    max_h = max(horizons)\n",
        "    for i in range(0, len(data)-lookback-max_h+1, stride):\n",
        "      X.append(data[i:i+lookback,:])\n",
        "      y.append(np.stack([data[i+lookback+h-1,:] for h in horizons], axis=0))\n",
        "    return np.array(X, np.float32), np.array(y, np.float32)\n",
        "\n",
        "# ======================================= Data Preparing =========================================================\n",
        "train_data = train_flow_speed.values\n",
        "val_data = val_flow_speed.values\n",
        "test_data = test_flow_speed.values\n",
        "\n",
        "scaler = MinMaxScaler((0,1))\n",
        "train_data_scaled = scaler.fit_transform(train_data)\n",
        "val_data_scaled = scaler.transform(val_data)\n",
        "test_data_scaled = scaler.transform(test_data)\n",
        "\n",
        "X_train,y_train = make_dataset(train_data_scaled, lookback=LOOKBACK, horizons=HORIZONS)\n",
        "X_val,y_val = make_dataset(val_data_scaled, lookback=LOOKBACK, horizons=HORIZONS)\n",
        "X_test,y_test = make_dataset(test_data_scaled, lookback=LOOKBACK, horizons=HORIZONS)\n",
        "\n",
        "# transform numpy to tensor & float32\n",
        "X_train = torch.from_numpy(X_train).float()   # [N,L,2*8]\n",
        "y_train = torch.from_numpy(y_train).float()   # [N,H,2*8]\n",
        "X_val = torch.from_numpy(X_val).float()\n",
        "y_val = torch.from_numpy(y_val).float()\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "train_dataset = TensorDataset(X_train,y_train)\n",
        "val_dataset = TensorDataset(X_val,y_val)\n",
        "test_dataset = TensorDataset(X_test,y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=BATCH,shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset,batch_size=BATCH,shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset,batch_size=BATCH,shuffle=False)\n",
        "\n",
        "\n",
        "# ====================== Instantiate a model & Set Loss Fcunction & Optimzier ==============================\n",
        "D = train_flow_speed.shape[1]  # number of portals * 2 (two features: flow & speed)\n",
        "lstm_all = SimpleLSTM(input_size=D, hidden_size=HIDDEN, num_layers=1, horizon=len(HORIZONS), out_size=D)\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = optim.Adam(lstm_all.parameters(),lr=LR,weight_decay=WD)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "  optimizer, mode=\"min\", factor=0.5, patience=3, min_lr=1e-6\n",
        ")\n",
        "\n",
        "# ===================== Training and test loop ==================================================================\n",
        "train_losses, val_losses = [], []\n",
        "best_val, best_state, wait = float(\"inf\"), None, 0\n",
        "patience_es = 5\n",
        "\n",
        "start_time = timer()\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  print(f\"Epoch: {epoch}\\n---------\")\n",
        "  train_loss = train_step(lstm_all,train_dataloader,loss_fn,optimizer,device)\n",
        "  val_loss = test_step(lstm_all,val_dataloader,loss_fn,device)\n",
        "  scheduler.step(val_loss)\n",
        "  # print(\"current lr:\", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)     # for plot\n",
        "\n",
        "  # Early Stopping\n",
        "  if val_loss < best_val - 1e-6:\n",
        "    best_val, best_state, wait = val_loss, copy.deepcopy(lstm_all.state_dict()), 0\n",
        "  else:\n",
        "    wait += 1\n",
        "    if wait >= patience_es:\n",
        "      print(f\"Early stopping triggered at epoch {epoch}. Best val loss = {best_val:.5f}\")\n",
        "      break\n",
        "\n",
        "if best_state is not None:\n",
        "  lstm_all.load_state_dict(best_state)\n",
        "\n",
        "end_time = timer()\n",
        "total_time = print_train_time(start_time, end_time, device=device)\n",
        "\n",
        "### Visualize\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, marker = \"o\", label=\"Train loss\")\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, marker = \"s\", label=\"Val loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss(MAE)\")\n",
        "plt.title(\"Train & Val Loss vs. Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CxjxWqZl8jQQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(lstm_all.state_dict(), \"lstm_model.pth\")"
      ],
      "metadata": {
        "id": "bId4-xhivvsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Test evaluation\n",
        "metrics,y_true,y_pred = eval_model(model=lstm_all,\n",
        "      data_loader=test_dataloader,\n",
        "      loss_fn=loss_fn,\n",
        "      device=device,\n",
        "      scaler=scaler,\n",
        "      horizons=HORIZONS,\n",
        "      feature_names=feature_cols)\n",
        "print(metrics[\"MAE_overall\"], metrics[\"RMSE_overall\"])\n",
        "metrics[\"per_step_df\"]"
      ],
      "metadata": {
        "id": "srjIU10EBzTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics[\"per_step_df\"].to_csv(\"lstm_evaluation_results.csv\")"
      ],
      "metadata": {
        "id": "TvxC689D4BRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN + LSTM"
      ],
      "metadata": {
        "id": "zjJhz4o-uiWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train_df.csv\")\n",
        "val_df = pd.read_csv(\"val_df.csv\")\n",
        "test_df = pd.read_csv(\"test_df.csv\")\n",
        "train_df[\"datetime\"] = pd.to_datetime(train_df[\"datetime\"])\n",
        "val_df[\"datetime\"] = pd.to_datetime(val_df[\"datetime\"])\n",
        "test_df[\"datetime\"] = pd.to_datetime(test_df[\"datetime\"])"
      ],
      "metadata": {
        "id": "rqYrYtqCOi3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portal_ids = [\"E4S 55,620\", \"E4S 56,160\", \"E4S 56,490\", \"E4S 56,780\",\n",
        "        \"E4S 57,055\", \"E4S 57,435\", \"E4S 57,820\", \"E4S 58,140\"]"
      ],
      "metadata": {
        "id": "V_RzODHB8EjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def id_to_pos(pid: str) -> int:\n",
        "  \"\"\"\n",
        "    Transform the numbers in the portal id into a \"meter mark\",\n",
        "    for calculating distance later\n",
        "  \"\"\"\n",
        "  s = pid.replace(\"E4S\", \"\").replace(\",\", \"\")\n",
        "  s = s.strip()\n",
        "  return int(s)"
      ],
      "metadata": {
        "id": "vh8Yj35qD4HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = np.array([id_to_pos(p) for p in portal_ids], dtype=np.float32)\n",
        "d_nn = np.abs(np.diff(pos))\n",
        "# choose the 0.5*median of adjacent distances as sigma, for calculating Gaussian kernel (weighted adjacency)\n",
        "median = np.median(d_nn)\n",
        "sigma = median * 0.5\n",
        "sigma"
      ],
      "metadata": {
        "id": "ED77H_4IEMFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_gaussian(d, sigma):\n",
        "  \"\"\"\n",
        "    Gaussian kernel as weight function\n",
        "  \"\"\"\n",
        "  return np.exp(-(d**2)/(2*sigma**2))"
      ],
      "metadata": {
        "id": "5qQYoqVaIHFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_Ahat(portal_ids, sigma, device:torch.device=device):\n",
        "\n",
        "  N = len(portal_ids)\n",
        "  pos = np.array([id_to_pos(p) for p in portal_ids], dtype=np.float32)\n",
        "  order = np.argsort(-pos)\n",
        "\n",
        "  # build an adjacency matrix A\n",
        "  A = np.zeros((N,N),dtype=np.float32)\n",
        "  for k in range(N-1):\n",
        "    up, down = order[k], order[k+1]\n",
        "    d = np.abs(pos[up] - pos[down])\n",
        "    w = weight_gaussian(d, sigma)\n",
        "    A[down, up] = w\n",
        "\n",
        "  # A self_connections\n",
        "  A_wavy = A + np.eye(N, dtype=np.float32)\n",
        "  # D: degree matrix\n",
        "  D = A_wavy.sum(axis=1)\n",
        "  # D^(-1/2)\n",
        "  D_inv_sqrt_vec = np.power(D, -0.5, where=D>0).astype(np.float32)\n",
        "  D_inv_sqrt_mat = np.diag(D_inv_sqrt_vec)\n",
        "  # A_hat\n",
        "  A_hat_t = D_inv_sqrt_mat @ A_wavy @ D_inv_sqrt_mat\n",
        "  A_hat_t = torch.from_numpy(A_hat_t).float()\n",
        "  A_hat_t = A_hat_t.to(device)\n",
        "  return A_hat_t"
      ],
      "metadata": {
        "id": "C4WeXb6wNREF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerGCN(nn.Module):\n",
        "  def __init__(self, A_hat: torch.Tensor, in_feat, hidden_size, out_feat, dropout: float = 0.0):\n",
        "    super().__init__()\n",
        "    self.in_feat = in_feat\n",
        "    self.hidden_size = hidden_size\n",
        "    self.out_feat = out_feat\n",
        "    self.register_buffer(\"A_hat\", A_hat)\n",
        "\n",
        "    self.lin1 = nn.Linear(in_features=in_feat, out_features=hidden_size)\n",
        "    self.lin2 = nn.Linear(in_features=hidden_size, out_features=out_feat)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "  def _gcn_once(self, X: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "      H1: ReLU(A_hat@X@W0)\n",
        "      Y: A_hat@H1@W1\n",
        "      X：[B，N, Feature_in]: (batch_size, nodes, input_features)\n",
        "      Y：[B, N, Feature_out]: (batch_size, nodes, output_features)\n",
        "      one time step\n",
        "    \"\"\"\n",
        "    H = torch.matmul(self.A_hat, X)              # [B,N,F_in]\n",
        "    H = self.lin1(H)                      # [B,N,hidden]\n",
        "    H = self.relu(H)\n",
        "    H = self.drop(H)\n",
        "    Y = torch.matmul(self.A_hat, H)              # [B,N,hidden]\n",
        "    Y = self.lin2(Y)                      # [B,N,F_out]\n",
        "    return Y\n",
        "\n",
        "  def forward(self, X:torch.Tensor) -> torch.Tensor:\n",
        "    if X.dim() == 3: #[B,N,F]\n",
        "      return self._gcn_once(X)\n",
        "    elif X.dim() == 4: #[B,L,N,F]\n",
        "      B, L, N, F = X.shape\n",
        "      X = X.reshape(B*L, N, F)\n",
        "      Y = self._gcn_once(X) #[B*L,N,F]\n",
        "      Y = Y.reshape(B,L,N,self.out_feat)\n",
        "      return Y\n",
        "    else:\n",
        "      raise ValueError(\"Input dimension should be 3[B,N,F] or 4[B,L,N,F]\")"
      ],
      "metadata": {
        "id": "BIiSnIgr9nic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerGCN_LSTM(nn.Module):\n",
        "  def __init__(self, A_hat: torch.Tensor, in_feat: int=2, gcn_hidden: int=16, gcn_out: int=8, gcn_dropout: float=0.2,\n",
        "         lstm_hidden: int=96, lstm_layers=1, out_feat: int=2, horizon: int=15, N: int=8):\n",
        "    super().__init__()\n",
        "    self.in_feat = in_feat\n",
        "    self.gcn_out = gcn_out\n",
        "    self.out_feat = out_feat\n",
        "    self.horizon = horizon\n",
        "    self.N = N\n",
        "\n",
        "    self.gcn = TwoLayerGCN(A_hat=A_hat, in_feat=in_feat, hidden_size=gcn_hidden, out_feat=gcn_out, dropout=gcn_dropout)\n",
        "    # self.in_drop = nn.Dropout(0.1)\n",
        "    self.lstm = nn.LSTM(input_size=N*gcn_out, hidden_size=lstm_hidden, num_layers=lstm_layers, batch_first=True)\n",
        "    # self.out_drop = nn.Dropout(0.1)\n",
        "    self.linear = nn.Linear(in_features=lstm_hidden,\n",
        "                 out_features=horizon*N*out_feat)\n",
        "\n",
        "  def forward(self,x:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    x: [B, L, N*F(2*N)]\n",
        "    -> [B, L, N, F] GCN\n",
        "    -> [B, L, N*F] LSTM\n",
        "    \"\"\"\n",
        "    B, L, D = x.shape\n",
        "    assert D == self.N * self.in_feat\n",
        "    # [B,L,N*F] --> [B,L,N,F] feed to GCN\n",
        "    x = x.reshape(B,L,self.N,self.in_feat)\n",
        "    g = self.gcn(x) # [B,L,N,GCN_OUT]\n",
        "    g = g.reshape(B,L,self.N*self.gcn_out)\n",
        "    # g = self.in_drop(g)\n",
        "    lstm_out, (h_n,c_n) = self.lstm(g)\n",
        "    # lstm_out = self.out_drop(lstm_out)\n",
        "    y = self.linear(lstm_out[:,-1,:])   # [B,H*N*F]\n",
        "    y = y.view(-1,self.horizon,self.N,self.out_feat) # [B,H,N,F]\n",
        "    y = y.reshape(B,self.horizon,self.N*self.out_feat) # [B,H,N*F]\n",
        "    return y"
      ],
      "metadata": {
        "id": "uPg_P6oodDLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "HORIZONS = list(range(1,16))\n",
        "LOOKBACK = 60\n",
        "BATCH = 32\n",
        "GCN_HIDDEN = 16\n",
        "GCN_OUT = 8\n",
        "GCN_DROPOUT = 0.2\n",
        "LSTM_HIDDEN = 96\n",
        "LR = 5e-4\n",
        "WD = 1e-4\n",
        "EPOCHS = 50\n",
        "N = len(portal_ids)\n",
        "A_hat = build_Ahat(portal_ids, sigma, device=device)\n",
        "\n",
        "def make_dataset(data, lookback=60, horizons=None, stride=1):\n",
        "  X, y = [], []\n",
        "  max_h = max(horizons)\n",
        "  for i in range(0, len(data)-lookback-max_h+1, stride):\n",
        "    X.append(data[i:i+lookback,:])\n",
        "    y.append(np.stack([data[i+lookback+h-1,:] for h in horizons], axis=0))\n",
        "  return np.array(X, np.float32), np.array(y, np.float32)\n",
        "\n",
        "# ======================================= Data Preparing =========================================================\n",
        "train_data = train_flow_speed.values\n",
        "val_data = val_flow_speed.values\n",
        "test_data = test_flow_speed.values\n",
        "\n",
        "scaler = MinMaxScaler((0,1))\n",
        "train_data_scaled = scaler.fit_transform(train_data)\n",
        "val_data_scaled = scaler.transform(val_data)\n",
        "test_data_scaled = scaler.transform(test_data)\n",
        "\n",
        "X_train,y_train = make_dataset(train_data_scaled, lookback=LOOKBACK, horizons=HORIZONS)\n",
        "X_val,y_val = make_dataset(val_data_scaled, lookback=LOOKBACK, horizons=HORIZONS)\n",
        "X_test,y_test = make_dataset(test_data_scaled, lookback=LOOKBACK, horizons=HORIZONS)\n",
        "\n",
        "# transform numpy to tensor & float32\n",
        "X_train = torch.from_numpy(X_train).float()   # [N,L,2*8]\n",
        "y_train = torch.from_numpy(y_train).float()   # [N,H,2*8]\n",
        "X_val = torch.from_numpy(X_val).float()\n",
        "y_val = torch.from_numpy(y_val).float()\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "train_dataset = TensorDataset(X_train,y_train)\n",
        "val_dataset = TensorDataset(X_val,y_val)\n",
        "test_dataset = TensorDataset(X_test,y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=BATCH,shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset,batch_size=BATCH,shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset,batch_size=BATCH,shuffle=False)\n",
        "\n",
        "\n",
        "# ====================== Instantiate a model & Set Loss Fcunction & Optimzier ==============================\n",
        "\n",
        "gcn_lstm = TwoLayerGCN_LSTM(A_hat=A_hat, in_feat=2, gcn_hidden=GCN_HIDDEN, gcn_out=GCN_OUT, gcn_dropout=GCN_DROPOUT,\n",
        "         lstm_hidden=LSTM_HIDDEN, lstm_layers=1, out_feat=2, horizon=len(HORIZONS), N=N)\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = optim.Adam(gcn_lstm.parameters(),lr=LR,weight_decay=WD)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "  optimizer, mode=\"min\", factor=0.5, patience=3, min_lr=1e-6\n",
        ")\n",
        "\n",
        "# ===================== Training and test loop ==================================================================\n",
        "train_losses, val_losses = [], []\n",
        "best_val, best_state, wait = float(\"inf\"), None, 0\n",
        "patience_es = 5\n",
        "\n",
        "start_time = timer()\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  print(f\"Epoch: {epoch}\\n---------\")\n",
        "  train_loss = train_step(gcn_lstm,train_dataloader,loss_fn,optimizer,device)\n",
        "  val_loss = test_step(gcn_lstm,val_dataloader,loss_fn,device)\n",
        "  scheduler.step(val_loss)\n",
        "  # print(\"current lr:\", optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)     # for plot\n",
        "\n",
        "  # Early Stopping\n",
        "  if val_loss < best_val - 1e-6:\n",
        "    best_val, best_state, wait = val_loss, copy.deepcopy(gcn_lstm.state_dict()), 0\n",
        "  else:\n",
        "    wait += 1\n",
        "    if wait >= patience_es:\n",
        "      print(f\"Early stopping triggered at epoch {epoch}. Best val loss = {best_val:.5f}\")\n",
        "      break\n",
        "\n",
        "if best_state is not None:\n",
        "  gcn_lstm.load_state_dict(best_state)\n",
        "\n",
        "end_time = timer()\n",
        "total_time = print_train_time(start_time, end_time, device=device)\n",
        "\n",
        "### Visualize\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, marker = \"o\", label=\"Train loss\")\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, marker = \"s\", label=\"Val loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss(MAE)\")\n",
        "plt.title(\"Train & Val Loss vs. Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x0MhQNywV2JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(gcn_lstm.state_dict(), \"gcn_lstm_model.pth\")"
      ],
      "metadata": {
        "id": "q9j1-UwTwF76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final test evaluation\n",
        "metrics,y_true,y_pred = eval_model(model=gcn_lstm,\n",
        "      data_loader=test_dataloader,\n",
        "      loss_fn=loss_fn,\n",
        "      device=device,\n",
        "      scaler=scaler,\n",
        "      horizons=HORIZONS,\n",
        "      feature_names=feature_cols)\n",
        "print(metrics[\"MAE_overall\"], metrics[\"RMSE_overall\"])\n",
        "metrics[\"per_step_df\"]"
      ],
      "metadata": {
        "id": "zsegtbqddt5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics[\"per_step_df\"].to_csv(\"gcn_lstm_evaluation_results.csv\")"
      ],
      "metadata": {
        "id": "nVa381QS0dUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XHEu3Qj5nmK8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}